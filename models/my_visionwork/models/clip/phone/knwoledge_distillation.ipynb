{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [03:36<00:00, 785825.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Epoch [1/10], Loss: 0.8739\n",
      "Epoch [2/10], Loss: 0.9472\n",
      "Epoch [3/10], Loss: 0.8050\n",
      "Epoch [4/10], Loss: 0.6588\n",
      "Epoch [5/10], Loss: 0.9096\n",
      "Epoch [6/10], Loss: 0.7311\n",
      "Epoch [7/10], Loss: 0.6789\n",
      "Epoch [8/10], Loss: 0.5437\n",
      "Epoch [9/10], Loss: 0.6746\n",
      "Epoch [10/10], Loss: 0.6755\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "teacher_model = models.resnet18(pretrained=True)\n",
    "teacher_model.fc = nn.Linear(512, 10)\n",
    "teacher_model.eval()  \n",
    "\n",
    "student_model = SmallCNN()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, T):\n",
    "    teacher_probs = nn.functional.softmax(teacher_logits / T, dim=1)\n",
    "    student_probs = nn.functional.log_softmax(student_logits / T, dim=1)\n",
    "    return nn.functional.kl_div(student_probs, teacher_probs, reduction='batchmean') * (T * T)\n",
    "\n",
    "num_epochs = 10\n",
    "temperature = 5.0 \n",
    "alpha = 0.5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    student_model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through teacher and student models\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(images)\n",
    "        \n",
    "        student_logits = student_model(images)\n",
    "        \n",
    "        # Calculate the distillation loss\n",
    "        distill_loss = distillation_loss(student_logits, teacher_logits, temperature)\n",
    "        \n",
    "        # Calculate the classification loss (standard supervised learning loss)\n",
    "        classification_loss = criterion(student_logits, labels)\n",
    "        \n",
    "        # Combined loss: a weighted sum of distillation loss and classification loss\n",
    "        loss = alpha * distill_loss + (1 - alpha) * classification_loss\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get predictions\n",
    "        teacher_preds = torch.argmax(teacher_logits, dim=1)\n",
    "        student_preds = torch.argmax(student_logits, dim=1)\n",
    "\n",
    "        # Print predictions for the current batch\n",
    "        for i in range(len(images)):\n",
    "            print(f\"Image {i+1}:\")\n",
    "            print(f\"  Teacher Prediction: {teacher_preds[i].item()}, Student Prediction: {student_preds[i].item()}\")\n",
    "    \n",
    "        break\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 8\n",
      "Predicted Class Name: ship\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the student_model is already defined and trained\n",
    "student_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define the same transformation used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to match input size of student model\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Function to preprocess and predict on a single image\n",
    "def predict_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)  # Apply the transformations\n",
    "    image = image.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        output = student_model(image)  # Forward pass through the model\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = torch.argmax(output, dim=1).item()  # Get the index of the max log-probability\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "image_path = '/home/ajeet/code/testing/data/person1.jpg'  # Replace with your image path\n",
    "predicted_class = predict_image(image_path)\n",
    "\n",
    "# Display the predicted class\n",
    "print(f'Predicted Class: {predicted_class}')\n",
    "\n",
    "CIFAR10_CLASSES = [\n",
    "    \"plane\", \"car\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# Use the predicted_class index to get the class name\n",
    "predicted_class_name = CIFAR10_CLASSES[predicted_class]\n",
    "print(f'Predicted Class Name: {predicted_class_name}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajeet/codework/venv_visionwork/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ajeet/codework/venv_visionwork/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "teacher_model = models.resnet18(pretrained=True)\n",
    "teacher_model.fc = nn.Linear(512, 10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "teacher_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Loss: 0.0540\n",
      "Epoch [2/8], Loss: 0.0458\n",
      "Epoch [3/8], Loss: 0.0463\n",
      "Epoch [4/8], Loss: 0.0419\n",
      "Epoch [5/8], Loss: 0.0403\n",
      "Epoch [6/8], Loss: 0.0341\n",
      "Epoch [7/8], Loss: 0.0336\n",
      "Epoch [8/8], Loss: 0.0364\n",
      "Fine-tuned teacher model saved!\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(teacher_model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 8\n",
    "teacher_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        outputs = teacher_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "torch.save(teacher_model.state_dict(), 'fine_tuned_teacher_model_2.pth')\n",
    "print(\"Fine-tuned teacher model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class index: 3\n",
      "Predicted class: cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17899/2273118074.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher_model.load_state_dict(torch.load('fine_tuned_teacher_model_2.pth'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "teacher_model = models.resnet18(pretrained=False)\n",
    "teacher_model.fc = nn.Linear(512, 10)\n",
    "teacher_model.load_state_dict(torch.load('fine_tuned_teacher_model_2.pth'))\n",
    "teacher_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher_model.to(device)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "def infer(image_path):\n",
    "    image = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "image_path = '/home/ajeet/Downloads/cat.jpeg'\n",
    "predicted_class = infer(image_path)\n",
    "print(f'Predicted class index: {predicted_class}')\n",
    "\n",
    "cifar10_classes = [\n",
    "    \"plane\", \"car\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "predicted_class_name = cifar10_classes[predicted_class]\n",
    "print(f'Predicted class: {predicted_class_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the fine-tuned model on the test dataset: 83.18%\n"
     ]
    }
   ],
   "source": [
    "# def test(model, test_loader, device):\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "#     return accuracy\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = teacher_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # break\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the fine-tuned model on the test dataset: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 6.2819\n",
      "Epoch [2/10], Loss: 8.9293\n",
      "Epoch [3/10], Loss: 5.3986\n",
      "Epoch [4/10], Loss: 5.4600\n",
      "Epoch [5/10], Loss: 4.0815\n",
      "Epoch [6/10], Loss: 7.2221\n",
      "Epoch [7/10], Loss: 6.2185\n",
      "Epoch [8/10], Loss: 4.2522\n",
      "Epoch [9/10], Loss: 4.8399\n",
      "Epoch [10/10], Loss: 3.0830\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# teacher_model = models.resnet18(pretrained=True)\n",
    "# teacher_model.fc = nn.Linear(512, 10)\n",
    "# teacher_model.eval()\n",
    "\n",
    "student_model = SmallCNN()\n",
    "student_model.to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, T):\n",
    "    teacher_probs = nn.functional.softmax(teacher_logits / T, dim=1)\n",
    "    student_probs = nn.functional.log_softmax(student_logits / T, dim=1)\n",
    "    return nn.functional.kl_div(student_probs, teacher_probs, reduction='batchmean') * (T * T)\n",
    "\n",
    "num_epochs = 10\n",
    "temperature = 5.0\n",
    "alpha = 0.5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    student_model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(images)\n",
    "        \n",
    "        student_logits = student_model(images)\n",
    "        \n",
    "        distill_loss = distillation_loss(student_logits, teacher_logits, temperature)\n",
    "        classification_loss = criterion(student_logits, labels)\n",
    "        \n",
    "        loss = alpha * distill_loss + (1 - alpha) * classification_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        teacher_preds = torch.argmax(teacher_logits, dim=1)\n",
    "        student_preds = torch.argmax(student_logits, dim=1)\n",
    "\n",
    "        # for i in range(len(images)):\n",
    "        #     print(f\"Image {i+1}:\")\n",
    "        #     print(f\"  Teacher Prediction: {teacher_preds[i].item()}, Student Prediction: {student_preds[i].item()}\")\n",
    "    \n",
    "        # break\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((32, 32)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "# ])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 1\n",
      "Predicted Class Name: car\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "student_model.eval() \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = student_model(image)\n",
    "\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "    return predicted_class\n",
    "\n",
    "image_path = '/home/ajeet/Downloads/car.jpeg'\n",
    "predicted_class = predict_image(image_path)\n",
    "\n",
    "print(f'Predicted Class: {predicted_class}')\n",
    "\n",
    "CIFAR10_CLASSES = [\n",
    "    \"plane\", \"car\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "predicted_class_name = CIFAR10_CLASSES[predicted_class]\n",
    "print(f'Predicted Class Name: {predicted_class_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the student model on the test dataset: 71.96%\n"
     ]
    }
   ],
   "source": [
    "student_model.to(device)\n",
    "student_model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader: \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = student_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the student model on the test dataset: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6865\n",
      "Model size in memory: 1.02 MB\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in student_model.parameters())\n",
    "param_size = 4\n",
    "\n",
    "print(total_params /100000)\n",
    "total_size = total_params * param_size / (1024 * 1024)\n",
    "print(f'Model size in memory: {total_size:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111.81642\n",
      "Model size in memory: 42.65 MB\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "param_size = 4\n",
    "\n",
    "print(total_params /100000)\n",
    "total_size = total_params * param_size / (1024 * 1024)\n",
    "print(f'Model size in memory: {total_size:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05088\n",
      "Model size in memory: 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "student_model.cpu()\n",
    "student_model.eval() \n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    student_model,\n",
    "    {nn.Linear},   \n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in quantized_model.parameters())\n",
    "param_size = 4\n",
    "\n",
    "print(total_params /100000)\n",
    "total_size = total_params * param_size / (1024 * 1024)\n",
    "print(f'Model size in memory: {total_size:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): DynamicQuantizedLinear(in_features=2048, out_features=128, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (fc2): DynamicQuantizedLinear(in_features=128, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of quantized model: tensor([[ -0.9451,  -1.4627,  -9.8652,  -4.8697, -18.1972, -22.9591,   8.9231,\n",
      "          -5.9368,  -0.3267,  14.9413]], grad_fn=<WarnNotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "quantized_output = quantized_model(dummy_input)\n",
    "print(f\"Output of quantized model: {quantized_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the student model on the test dataset: 70.75%\n"
     ]
    }
   ],
   "source": [
    "# quantized_model.to(device)\n",
    "# quantized_model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader: \n",
    "        images, labels = images, labels\n",
    "        \n",
    "        outputs = quantized_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the student model on the test dataset: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/10], Loss: 2.1874\n",
      "Epoch [2/10], Loss: 1.7820\n",
      "Epoch [3/10], Loss: 1.7328\n",
      "Epoch [4/10], Loss: 1.7014\n",
      "Epoch [5/10], Loss: 1.5585\n",
      "Epoch [6/10], Loss: 1.3253\n",
      "Epoch [7/10], Loss: 1.4570\n",
      "Epoch [8/10], Loss: 1.5063\n",
      "Epoch [9/10], Loss: 1.6937\n",
      "Epoch [10/10], Loss: 1.4680\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# teacher_model = models.resnet18(pretrained=True)\n",
    "# teacher_model.fc = nn.Linear(512, 10)\n",
    "# teacher_model.eval()\n",
    "\n",
    "student_model = SmallCNN()\n",
    "student_model.to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=0.0001)\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, T):\n",
    "    teacher_probs = nn.functional.softmax(teacher_logits / T, dim=1)\n",
    "    student_probs = nn.functional.log_softmax(student_logits / T, dim=1)\n",
    "    return nn.functional.kl_div(student_probs, teacher_probs, reduction='batchmean') * (T * T)\n",
    "\n",
    "num_epochs = 10\n",
    "temperature = 5.0\n",
    "alpha = 0.5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    student_model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(images)\n",
    "        \n",
    "        student_logits = student_model(images)\n",
    "        \n",
    "        # distill_loss = distillation_loss(student_logits, teacher_logits, temperature)\n",
    "        # classification_loss = criterion(student_logits, labels)\n",
    "        \n",
    "        # loss = alpha * distill_loss + (1 - alpha) * classification_loss\n",
    "\n",
    "        soft_targets = nn.functional.softmax(teacher_logits / 2, dim=-1)\n",
    "        soft_prob = nn.functional.log_softmax(student_logits / 2, dim=-1)\n",
    "\n",
    "        # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "        soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (2**2)\n",
    "\n",
    "        # Calculate the true label loss\n",
    "        label_loss = criterion(student_logits, labels)\n",
    "\n",
    "        # Weighted sum of the two losses\n",
    "        loss = 0.10 * soft_targets_loss + 0.90 * label_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        teacher_preds = torch.argmax(teacher_logits, dim=1)\n",
    "        student_preds = torch.argmax(student_logits, dim=1)\n",
    "\n",
    "        # for i in range(len(images)):\n",
    "        #     print(f\"Image {i+1}:\")\n",
    "        #     print(f\"  Teacher Prediction: {teacher_preds[i].item()}, Student Prediction: {student_preds[i].item()}\")\n",
    "    \n",
    "        # break\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Accuracy of the student model on the test dataset: 56.40%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((32, 32)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "# ])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "student_model.to(device)\n",
    "student_model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader: \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = student_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the student model on the test dataset: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/25], Loss: 1.4256\n",
      "Epoch [2/25], Loss: 1.1326\n",
      "Epoch [3/25], Loss: 0.6523\n",
      "Epoch [4/25], Loss: 0.3514\n",
      "Epoch [5/25], Loss: 0.4743\n",
      "Epoch [6/25], Loss: 0.6629\n",
      "Epoch [7/25], Loss: 0.2457\n",
      "Epoch [8/25], Loss: 0.5723\n",
      "Epoch [9/25], Loss: 0.3800\n",
      "Epoch [10/25], Loss: 0.4427\n",
      "Epoch [11/25], Loss: 0.1900\n",
      "Epoch [12/25], Loss: 0.3048\n",
      "Epoch [13/25], Loss: 0.7931\n",
      "Epoch [14/25], Loss: 0.0563\n",
      "Epoch [15/25], Loss: 0.1185\n",
      "Epoch [16/25], Loss: 0.1702\n",
      "Epoch [17/25], Loss: 0.1869\n",
      "Epoch [18/25], Loss: 0.2829\n",
      "Epoch [19/25], Loss: 0.2178\n",
      "Epoch [20/25], Loss: 0.1647\n",
      "Epoch [21/25], Loss: 0.0865\n",
      "Epoch [22/25], Loss: 0.1052\n",
      "Epoch [23/25], Loss: 0.0546\n",
      "Epoch [24/25], Loss: 0.0073\n",
      "Epoch [25/25], Loss: 0.3126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# teacher_model = models.resnet18(pretrained=True)\n",
    "# teacher_model.fc = nn.Linear(512, 10)  # Adjusting output for CIFAR-10\n",
    "# teacher_model.eval()  # Set to evaluation mode\n",
    "\n",
    "device = \"cuda\"\n",
    "student_model_without_distillation = SmallCNN()\n",
    "student_model_without_distillation.to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "r\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model_without_distillation.parameters(), lr=0.001)\n",
    "\n",
    "# def distillation_loss(student_logits, teacher_logits, T):\n",
    "#     teacher_probs = nn.functional.softmax(teacher_logits / T, dim=1)\n",
    "#     student_probs = nn.functional.log_softmax(student_logits / T, dim=1)\n",
    "#     return nn.functional.kl_div(student_probs, teacher_probs, reduction='batchmean') * (T * T)\n",
    "\n",
    "num_epochs = 25\n",
    "temperature = 5.0\n",
    "alpha = 0.5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    student_model_without_distillation.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        #     teacher_logits = teacher_model(images)\n",
    "        \n",
    "        student_logits = student_model_without_distillation(images)\n",
    "        \n",
    "        # distill_loss = distillation_loss(student_logits, teacher_logits, temperature)\n",
    "        \n",
    "        classification_loss = criterion(student_logits, labels)\n",
    "        \n",
    "        # loss = alpha * distill_loss + (1 - alpha) * classification_loss\n",
    "        loss = classification_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the student_model_without_distillation on the test dataset: 66.36%\n"
     ]
    }
   ],
   "source": [
    "student_model_without_distillation.to(device)\n",
    "student_model_without_distillation.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = student_model_without_distillation(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the student_model_without_distillation on the test dataset: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajeet/codework/venv_visionwork/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ajeet/codework/venv_visionwork/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: trailer truck\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "without_finetuned_model = models.resnet18(pretrained=True)\n",
    "without_finetuned_model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB') \n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def classify_image(image_path):\n",
    "    image_tensor = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        outputs = without_finetuned_model(image_tensor)\n",
    "        _, predicted_idx = torch.max(outputs, 1)\n",
    "    \n",
    "    labels = requests.get('https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt').text.splitlines()\n",
    "    predicted_class = labels[predicted_idx.item()]\n",
    "    return predicted_class\n",
    "\n",
    "image_path = '/home/ajeet/Downloads/truck.jpeg'\n",
    "predicted_class = classify_image(image_path)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "without_finetuned_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "without_finetuned_test_loader = torch.utils.data.DataLoader(without_finetuned_test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([128, 3, 256, 256])\n",
      "torch.Size([16, 3, 256, 256])\n",
      "Accuracy of the fine-tuned model on the test dataset: 0.00%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in without_finetuned_test_loader:\n",
    "        images, labels = images, labels\n",
    "        outputs = without_finetuned_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # break\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the without_finetuned model on the test dataset: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajeet/codework/venv_visionwork/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/5], Loss: 0.4101\n",
      "Epoch [2/5], Loss: 0.7097\n",
      "Epoch [3/5], Loss: 0.7318\n",
      "Epoch [4/5], Loss: 0.9238\n",
      "Epoch [5/5], Loss: 0.8046\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Teacher Model: Pre-trained ResNet for image classification\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])  # Remove the final layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():  # We don't update the teacher model\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        return x\n",
    "\n",
    "# Student Model: LSTM network to generate text (captions) based on image features\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_dim, vocab_size):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(feature_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, features):\n",
    "        # LSTM expects input of shape (batch_size, seq_length, input_size)\n",
    "        # We will assume sequence length of 1 for simplicity here\n",
    "        lstm_out, _ = self.lstm(features.unsqueeze(1))\n",
    "        output = self.fc(lstm_out.squeeze(1))  # Output shape: (batch_size, vocab_size)\n",
    "        return output\n",
    "\n",
    "# Example vocabulary size (10 classes for simplicity)\n",
    "vocab_size = 10\n",
    "teacher_model = TeacherModel().to(device)\n",
    "student_model = StudentModel(feature_dim=512, hidden_dim=256, vocab_size=vocab_size).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "\n",
    "# Transform and Data Loader for CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Dummy function to convert labels to words (for demonstration purposes)\n",
    "def label_to_word(label):\n",
    "    word_list = [\"cat\", \"dog\", \"car\", \"plane\", \"tree\", \"house\", \"person\", \"ship\", \"horse\", \"bird\"]\n",
    "    return word_list[label]\n",
    "\n",
    "# Training loop for Cross-Modal Distillation\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    student_model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass with teacher model to get image features\n",
    "        image_features = teacher_model(images)  # Shape: (batch_size, 512)\n",
    "        \n",
    "        # Forward pass with student model\n",
    "        student_outputs = student_model(image_features)  # Shape: (batch_size, vocab_size)\n",
    "        \n",
    "        # Convert labels to a compatible form (example: using class index as \"words\")\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Compute loss and optimize\n",
    "        loss = criterion(student_outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# After training, you could input image features and have the student generate \"captions\" based on these features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: frog\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to perform inference using the student model\n",
    "def infer(image, teacher_model, student_model, device):\n",
    "    # Set both models to evaluation mode\n",
    "    teacher_model.eval()\n",
    "    student_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Move image to device and pass through the teacher model to extract features\n",
    "        image = image.to(device)\n",
    "        image_features = teacher_model(image.unsqueeze(0))  # Add batch dimension\n",
    "        \n",
    "        # Pass the extracted features through the student model to generate output\n",
    "        output = student_model(image_features)\n",
    "        \n",
    "        # Get the predicted class (index of the max value)\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "        \n",
    "        # Map the predicted index to a word (label) using our dummy vocabulary\n",
    "        word_list = [\n",
    "            \"plane\", \"car\", \"bird\", \"cat\", \"deer\",\n",
    "            \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "        ]\n",
    "        predicted_word = word_list[predicted_class]\n",
    "        \n",
    "        return predicted_word\n",
    "\n",
    "# Example usage:\n",
    "# Get a sample image from the dataset\n",
    "sample_image, _ = train_dataset[0]  # Get the image (ignoring the label for inference)\n",
    "# sample_image = transform(sample_image)  # Apply the same transform used during training\n",
    "\n",
    "# Perform inference\n",
    "predicted_label = infer(sample_image, teacher_model, student_model, device)\n",
    "print(f\"Predicted label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Student model accuracy: 79.73%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Function to calculate accuracy of the student model\n",
    "def calculate_accuracy(student_model, teacher_model, test_loader, device):\n",
    "    student_model.eval()\n",
    "    teacher_model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Extract features from the teacher model\n",
    "            image_features = teacher_model(images)\n",
    "            \n",
    "            # Get outputs from the student model\n",
    "            student_outputs = student_model(image_features)\n",
    "\n",
    "            # Get the predicted class (index of the max value)\n",
    "            _, predicted = torch.max(student_outputs, dim=1)\n",
    "\n",
    "            # Update total and correct predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total * 100  # Calculate accuracy as a percentage\n",
    "    return accuracy\n",
    "\n",
    "# Transform for the test dataset (same as training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 test dataset\n",
    "test_dataset = CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Calculate accuracy of the student model\n",
    "accuracy = calculate_accuracy(student_model, teacher_model, test_loader, device)\n",
    "print(f'Student model accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.9105\n",
      "Model size in memory: 3.02 MB\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in student_model.parameters())\n",
    "param_size = 4\n",
    "\n",
    "print(total_params /100000)\n",
    "total_size = total_params * param_size / (1024 * 1024)\n",
    "print(f'Model size in memory: {total_size:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Predicted label: horse\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, vocab_size):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.cnn = models.resnet18(pretrained=False)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        \n",
    "        self.lstm = nn.LSTM(512, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        features = features.unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        output = self.fc(lstm_out.squeeze(1))\n",
    "        return output\n",
    "\n",
    "vocab_size = 10\n",
    "hidden_dim = 256\n",
    "\n",
    "student_model = StudentModel(hidden_dim=hidden_dim, vocab_size=vocab_size).to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "def infer(image, model, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        output = model(image.unsqueeze(0))\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "        CIFAR10_CLASSES = [\n",
    "            \"plane\", \"car\", \"bird\", \"cat\", \"deer\",\n",
    "            \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "        ]\n",
    "        predicted_label = CIFAR10_CLASSES[predicted_class]\n",
    "        \n",
    "        return predicted_label\n",
    "\n",
    "sample_image, _ = train_dataset[0] \n",
    "# sample_image = transform(sample_image)\n",
    "\n",
    "predicted_label = infer(sample_image, student_model, device)\n",
    "print(f\"Predicted label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.67562\n",
      "Model size in memory: 45.65 MB\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in student_model.parameters())\n",
    "param_size = 4\n",
    "\n",
    "print(total_params /100000)\n",
    "total_size = total_params * param_size / (1024 * 1024)\n",
    "print(f'Model size in memory: {total_size:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajeet/codework/venv_visionwork/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "2024-10-20 23:39:10.865419: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-20 23:39:11.625113: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-20 23:39:12.880869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Features: tensor([[ 1.7387e-02,  5.6551e-02, -4.6250e-02, -2.1587e-03,  7.5924e-02,\n",
      "         -7.3925e-02,  3.8801e-02,  9.4470e-02,  8.6250e-03, -2.7143e-02,\n",
      "          4.8001e-02, -2.4516e-02, -3.5613e-02, -6.2863e-02,  1.6915e-02,\n",
      "         -1.5899e-02, -2.9257e-02, -5.2187e-02,  6.5392e-02,  6.1481e-03,\n",
      "          2.9515e-02,  5.1625e-02, -6.3014e-02, -2.7322e-02,  1.5342e-02,\n",
      "          6.2930e-02,  8.1192e-02, -1.3159e-02,  6.3469e-02,  2.8556e-02,\n",
      "         -5.6184e-02,  7.8403e-02,  4.9809e-03, -3.2727e-02, -1.3405e-03,\n",
      "          9.7308e-02, -9.2236e-02, -1.9654e-02, -3.3392e-02, -1.1686e-02,\n",
      "          9.2331e-03,  2.2359e-02, -5.1954e-02, -9.2235e-03, -3.7786e-02,\n",
      "         -3.8870e-02, -8.2130e-02,  9.9372e-02,  3.7040e-02, -7.3556e-02,\n",
      "          7.9372e-03,  7.0264e-04, -6.5739e-02,  5.4766e-02, -4.7167e-02,\n",
      "          1.2790e-02, -2.3587e-02,  9.7568e-04,  1.2494e-02, -6.3058e-02,\n",
      "         -6.7019e-02, -5.2892e-02, -3.8356e-02,  5.5975e-03,  6.7959e-03,\n",
      "          9.9674e-02,  4.1282e-03, -5.9321e-02,  1.1679e-02,  4.8585e-03,\n",
      "         -1.6926e-02, -1.3854e-02, -5.5152e-02, -3.7179e-02, -7.1058e-02,\n",
      "          5.2890e-03, -1.0528e-02, -4.6328e-02, -4.2142e-02, -6.1532e-02,\n",
      "          3.0618e-02,  2.0958e-02, -4.0021e-02,  2.8732e-02,  3.1955e-02,\n",
      "          7.8064e-02, -3.7928e-02, -1.4519e-02,  7.5489e-03,  8.5104e-03,\n",
      "          1.0614e-02, -8.3975e-03,  6.7852e-03,  7.6521e-03,  2.8460e-03,\n",
      "          4.8935e-02, -4.3398e-03, -9.0550e-02, -1.2715e-01, -1.0138e-01,\n",
      "         -2.6646e-02, -5.5428e-02, -3.8412e-02, -1.0160e-02,  3.1399e-02,\n",
      "          2.4298e-02, -2.2455e-02, -4.6364e-02, -2.4465e-02,  5.0802e-02,\n",
      "          5.6940e-02, -6.9211e-03, -6.6858e-03, -1.3307e-03, -2.7569e-02,\n",
      "          1.7877e-02, -2.4535e-02, -5.8340e-02,  2.0119e-02, -2.6600e-02,\n",
      "         -8.8835e-02, -3.2365e-02, -5.2586e-02, -7.2374e-03,  1.4698e-02,\n",
      "         -8.1297e-03, -2.7818e-02, -7.0093e-02, -3.8419e-02,  7.2219e-03,\n",
      "          6.2112e-02, -7.4602e-02,  2.5048e-02,  1.2353e-02, -3.9762e-02,\n",
      "         -3.7056e-02,  7.7698e-02, -3.5253e-02, -9.9355e-02,  3.2501e-02,\n",
      "          3.3486e-02, -2.2166e-02,  2.3865e-02, -7.7414e-03,  6.2616e-03,\n",
      "          5.0748e-02,  7.7676e-02, -6.7734e-02,  2.4813e-02,  3.5631e-02,\n",
      "          1.5240e-02, -1.4390e-02,  2.5965e-02, -1.0332e-01, -9.7335e-02,\n",
      "         -5.6443e-02,  6.0968e-03, -3.5440e-02,  2.4885e-02,  2.3784e-03,\n",
      "          6.4538e-02, -2.6885e-02, -6.9407e-03,  3.2871e-02, -2.6778e-02,\n",
      "          1.5546e-02,  4.3709e-02,  1.1356e-02, -2.1871e-03,  7.0797e-02,\n",
      "         -6.9570e-03, -8.2789e-02, -7.9105e-04,  2.9001e-02,  7.2883e-02,\n",
      "         -2.7092e-02,  2.8414e-02, -9.8774e-03,  1.2276e-02, -7.0861e-02,\n",
      "         -7.5093e-03, -9.7592e-03,  5.3035e-03,  2.8473e-02, -4.8359e-02,\n",
      "         -9.8802e-02, -1.3148e-02, -1.9343e-02, -5.2030e-02,  2.0014e-02,\n",
      "         -1.0952e-02,  9.3505e-02,  5.5622e-02,  1.0602e-02, -3.5970e-02,\n",
      "         -5.5592e-03,  2.0137e-02,  1.1415e-02, -1.4819e-02, -7.9918e-03,\n",
      "          5.2326e-02, -2.5673e-02, -3.0461e-02, -1.2415e-01, -9.1891e-03,\n",
      "          4.3741e-02, -1.0668e-02,  1.5698e-02,  7.9098e-02,  3.5647e-02,\n",
      "         -4.1002e-04,  2.1061e-02, -2.7902e-02,  2.0341e-02, -1.9656e-02,\n",
      "         -7.7307e-02,  3.3582e-02, -4.4833e-02, -1.4979e-02,  7.1888e-02,\n",
      "          1.4189e-02, -3.3905e-02, -3.7079e-02, -4.5065e-02, -7.4610e-03,\n",
      "         -2.6153e-02, -1.2208e-02, -3.5296e-03, -9.2563e-03,  6.8593e-03,\n",
      "         -2.0417e-02, -2.3483e-02, -2.2961e-02,  6.5487e-03, -6.3985e-02,\n",
      "          1.5194e-02, -2.2051e-02, -7.6270e-02,  4.5427e-02, -2.6923e-02,\n",
      "         -7.5614e-02,  6.3118e-02,  5.8693e-02, -5.1079e-02, -8.2027e-04,\n",
      "          5.8296e-02, -4.1945e-02, -3.2675e-02, -7.9723e-03, -8.2783e-03,\n",
      "          4.6729e-02, -5.7453e-02, -6.4389e-02, -1.4267e-03, -2.1913e-03,\n",
      "          5.5937e-02,  1.2696e-02, -7.1602e-02, -1.8027e-02,  6.9588e-02,\n",
      "         -2.9743e-02, -1.8190e-03, -2.1824e-02,  2.3632e-02,  1.0383e-02,\n",
      "          1.9308e-03, -7.0009e-03, -6.6470e-02,  4.0084e-02, -1.7500e-03,\n",
      "         -8.8777e-02,  4.1155e-02,  5.2147e-03, -3.3895e-04,  1.1649e-04,\n",
      "         -7.5761e-02,  1.1995e-01, -1.3445e-02, -5.8304e-03,  1.9455e-02,\n",
      "          4.4566e-02,  2.4385e-02, -4.5091e-02,  7.2393e-02, -9.2531e-03,\n",
      "         -9.2300e-02, -2.3853e-02,  1.2700e-02,  7.1995e-03,  3.9256e-03,\n",
      "         -9.4475e-02,  3.5933e-02, -4.8457e-02, -6.5353e-02, -1.2651e-02,\n",
      "          6.8202e-02, -9.3004e-02,  7.4081e-03, -2.5113e-02,  2.9275e-02,\n",
      "         -4.8694e-02, -2.9383e-02, -5.4148e-02, -4.5644e-02, -8.5975e-03,\n",
      "          9.3041e-03, -4.2182e-02,  5.8637e-02,  1.1748e-02,  5.6864e-02,\n",
      "          2.1396e-02, -9.5758e-02,  8.0443e-02, -6.5124e-02, -5.8609e-02,\n",
      "         -2.6305e-02,  4.1081e-02, -1.5570e-02,  1.5433e-02, -1.0921e-02,\n",
      "          2.6912e-03, -1.3522e-02, -5.6923e-02,  5.6752e-04, -6.3818e-02,\n",
      "         -2.3680e-02, -3.9756e-02,  5.5713e-02,  2.5708e-02, -8.4094e-03,\n",
      "          1.0395e-01, -1.3505e-02, -6.6139e-03, -6.3714e-03,  3.8581e-02,\n",
      "         -1.5108e-03,  1.6637e-02, -4.8007e-02,  3.6918e-02,  5.9884e-03,\n",
      "         -2.2492e-03,  6.0304e-02, -2.4108e-02,  1.8726e-02,  6.5667e-02,\n",
      "          2.2142e-02,  3.1189e-02,  1.0573e-02, -4.2345e-02,  2.9503e-02,\n",
      "         -5.8745e-02,  3.6171e-03,  9.3983e-03,  3.8265e-02, -5.8159e-04,\n",
      "         -7.2847e-02,  7.9979e-02,  4.1018e-03,  3.1125e-02, -3.8828e-02,\n",
      "         -7.0040e-04,  3.5189e-02,  3.1307e-02,  5.5470e-02,  6.1595e-02,\n",
      "         -2.4179e-02, -1.8306e-02, -7.3217e-02,  1.0331e-01, -4.3810e-02,\n",
      "         -1.5435e-03, -2.9984e-02, -3.6324e-03, -6.1848e-02, -5.2926e-02,\n",
      "         -1.5490e-02,  4.2935e-02, -2.9331e-02, -3.4991e-02, -6.4812e-02,\n",
      "         -4.3581e-02,  8.1308e-02,  1.6353e-02, -6.4493e-02, -2.0179e-02,\n",
      "         -1.0357e-01, -2.9587e-03,  4.0792e-02,  3.8727e-02, -1.8296e-02,\n",
      "         -4.0325e-02, -3.7150e-02, -1.0729e-02, -1.8014e-02, -2.5648e-02,\n",
      "         -1.1645e-02,  7.3449e-04, -3.2502e-02, -5.8639e-03,  8.5606e-03,\n",
      "         -4.6584e-02, -3.6129e-02, -4.5174e-02,  1.9100e-03, -3.3591e-02,\n",
      "         -3.7955e-02, -4.4609e-02, -4.2388e-03, -2.0766e-03, -6.2329e-02,\n",
      "         -1.8519e-02,  7.4145e-02,  1.9534e-02, -7.5125e-02, -1.1595e-04,\n",
      "         -2.6868e-02, -8.3527e-02, -4.3431e-02, -8.9046e-02,  2.2277e-02,\n",
      "         -3.4184e-02,  1.6257e-02,  9.5169e-03, -4.7485e-02, -3.5558e-02,\n",
      "         -3.4034e-02,  5.3828e-02, -1.3783e-03,  3.4204e-02, -1.3609e-02,\n",
      "         -7.6518e-02,  6.1342e-02,  4.4264e-02,  6.6460e-02,  2.5935e-03,\n",
      "         -5.1776e-03, -5.7233e-02, -3.6615e-02,  8.1728e-02,  1.7085e-02,\n",
      "          9.8468e-03, -3.0744e-03, -1.6137e-02, -4.2013e-02,  5.2121e-03,\n",
      "          4.1779e-02, -4.8980e-02, -8.9652e-03,  1.9018e-02,  2.2578e-02,\n",
      "          5.4228e-02, -7.4410e-02,  1.5408e-02, -6.1308e-02,  2.3678e-02,\n",
      "         -3.7441e-02,  9.3984e-03, -1.7680e-02, -4.4086e-02,  5.5118e-02,\n",
      "         -2.6766e-02, -2.8845e-02, -1.1581e-02,  2.8447e-02,  1.1315e-02,\n",
      "          6.9045e-02, -2.1516e-03,  3.2942e-02,  1.9702e-02, -2.4482e-02,\n",
      "          2.6350e-02, -4.6713e-02, -5.5475e-03,  1.8441e-03, -5.7395e-02,\n",
      "         -1.7478e-02,  3.3394e-04,  2.8650e-02,  4.7576e-02,  4.7128e-02,\n",
      "          7.5327e-03,  1.1640e-02, -9.6633e-02,  1.7303e-02,  8.4491e-02,\n",
      "          5.3299e-02,  3.1978e-02,  6.0516e-02, -5.1877e-02,  2.5295e-03,\n",
      "          4.0848e-02,  6.8927e-02, -2.4644e-02, -3.4036e-02, -2.9597e-02,\n",
      "          5.5693e-02,  1.6306e-03,  1.1219e-03, -1.7162e-02,  4.5840e-02,\n",
      "         -1.4294e-02, -1.6032e-02, -2.0983e-02, -2.7904e-02,  2.3544e-03,\n",
      "         -9.5922e-02, -2.7806e-02, -6.2090e-02,  7.8162e-02, -3.8520e-02,\n",
      "          6.5695e-02, -3.2455e-02]])\n",
      "Image Features: tensor([[-2.9189e-02,  2.3827e-02,  8.7894e-02,  1.5097e-02, -2.9500e-02,\n",
      "         -5.0901e-03, -9.5673e-03, -4.0154e-02, -1.6169e-02, -5.8955e-02,\n",
      "         -6.8815e-02,  1.9152e-02,  2.9641e-02, -3.4138e-02, -8.6631e-03,\n",
      "         -8.6463e-02,  2.6390e-02, -3.8629e-02,  4.5245e-02, -6.5797e-04,\n",
      "         -4.6052e-02, -2.2792e-02,  2.1042e-02, -1.6644e-02,  9.9855e-03,\n",
      "         -2.1416e-02, -1.4782e-02, -3.7469e-03,  1.0886e-03,  2.2338e-02,\n",
      "          1.8295e-02, -1.2714e-02, -2.3868e-02,  7.7734e-06,  1.4838e-02,\n",
      "          4.9457e-02,  8.0487e-02, -7.6770e-03, -3.3810e-02, -5.5864e-03,\n",
      "         -4.5002e-02,  8.0838e-03, -7.5345e-02, -8.9257e-02,  9.4316e-02,\n",
      "         -7.6955e-02, -2.9246e-02,  5.6022e-03,  8.7493e-02,  6.5846e-02,\n",
      "         -3.1481e-02,  5.3023e-02,  1.7928e-02, -3.4063e-02, -3.0790e-02,\n",
      "          2.3728e-02,  3.6705e-02, -4.1342e-02,  7.3531e-02,  6.0881e-03,\n",
      "         -1.0566e-02,  3.9544e-02, -4.0780e-03,  2.1603e-02,  5.8797e-02,\n",
      "          1.9035e-02,  4.7426e-02,  3.6555e-02,  7.9578e-02,  4.9849e-02,\n",
      "          2.5172e-04, -1.0778e-02, -4.5186e-02,  2.7227e-02, -3.8431e-02,\n",
      "         -8.1240e-03,  1.6588e-02, -9.2756e-03,  6.4215e-02, -1.5519e-02,\n",
      "         -5.3944e-02, -3.3062e-02, -2.1955e-02,  1.7213e-02, -9.8846e-02,\n",
      "          1.2882e-02,  1.2186e-02, -2.6971e-03, -3.3748e-02, -5.1131e-02,\n",
      "         -7.1254e-02, -3.0901e-02,  2.8626e-02,  5.6444e-02,  7.1815e-02,\n",
      "          3.2055e-02,  1.9160e-02, -3.8925e-02,  1.7792e-02, -9.6720e-02,\n",
      "         -4.1683e-02,  9.1031e-02, -9.8819e-03,  1.3177e-02, -2.1562e-02,\n",
      "          1.3395e-02, -3.7334e-02, -2.5558e-02, -6.0061e-02, -5.2386e-02,\n",
      "          4.1128e-02, -5.8261e-02,  5.8227e-02, -4.8232e-02,  6.9529e-03,\n",
      "          2.7587e-02, -2.2175e-02,  8.2205e-03, -1.2684e-02,  9.8619e-03,\n",
      "         -5.5427e-02,  1.9407e-03,  3.6689e-02, -6.1484e-02, -5.3548e-02,\n",
      "         -6.0268e-02,  5.9514e-02,  3.1307e-02,  7.7471e-02, -1.9515e-02,\n",
      "          2.4662e-02,  1.8272e-02, -5.4828e-02,  7.5077e-03,  5.6065e-02,\n",
      "         -4.8138e-02,  3.4206e-02,  3.4353e-02,  2.4931e-02,  5.3057e-02,\n",
      "         -8.7595e-02, -2.8646e-02, -4.9896e-02,  2.0434e-03,  6.6997e-02,\n",
      "         -5.2361e-02, -3.2316e-03,  1.8795e-02, -1.2278e-02,  3.7981e-02,\n",
      "         -3.1684e-02,  4.5982e-02,  8.8773e-02, -3.2103e-02,  1.7857e-02,\n",
      "          2.1404e-03, -2.3673e-02, -3.2408e-02, -1.2700e-02, -6.1847e-02,\n",
      "         -2.0310e-02, -3.6853e-02,  1.3985e-02,  2.6131e-02,  4.9572e-02,\n",
      "         -3.2717e-02,  1.2356e-01, -8.6762e-04, -3.9625e-02,  4.0273e-02,\n",
      "          2.1734e-03, -2.1794e-02,  4.5138e-02,  9.5218e-03,  1.9778e-02,\n",
      "         -3.8952e-02,  1.1647e-02, -1.1436e-02,  3.6212e-02, -5.4836e-02,\n",
      "         -1.0691e-01, -5.5187e-02,  9.4053e-03, -2.9541e-03,  6.0026e-02,\n",
      "         -5.9972e-02,  2.7642e-02, -6.1564e-03,  6.2843e-02,  3.3859e-02,\n",
      "          3.8612e-02,  4.9839e-03, -4.2075e-02, -8.8156e-02, -2.4924e-03,\n",
      "          1.9653e-02,  1.5370e-02, -4.3229e-02, -2.0233e-02, -3.1429e-02,\n",
      "          3.2527e-02,  5.2361e-02,  2.2296e-02, -1.1495e-01,  2.6341e-02,\n",
      "          1.2200e-02, -5.0391e-02, -5.6253e-02, -2.7891e-02, -1.8919e-02,\n",
      "          7.2663e-02, -6.9360e-02, -4.1824e-02, -3.3813e-02,  1.3556e-02,\n",
      "         -1.4609e-02,  4.8120e-02,  2.0792e-03, -3.0728e-02,  6.8712e-02,\n",
      "         -3.6109e-02,  3.0454e-02, -4.4252e-04,  3.5126e-02,  1.4093e-02,\n",
      "          3.3077e-02, -9.2173e-03,  2.0205e-03, -4.1626e-02, -1.2687e-02,\n",
      "         -2.6443e-02, -1.1458e-02, -6.3860e-02,  7.6466e-03,  4.9928e-02,\n",
      "          4.2859e-02, -2.6026e-02, -9.0025e-03,  3.1093e-02, -4.6958e-02,\n",
      "          4.8576e-02, -3.5073e-02, -8.8098e-03, -2.5042e-02,  1.3932e-02,\n",
      "         -1.2989e-02, -8.2781e-02, -5.9771e-03, -2.9257e-02,  7.7469e-02,\n",
      "         -1.1514e-02,  1.5426e-02, -2.3694e-02, -3.3172e-02, -8.6269e-02,\n",
      "          2.3796e-02, -5.6062e-03,  4.6794e-02, -3.6250e-02, -2.1120e-03,\n",
      "         -1.1309e-02,  4.6134e-02, -1.6569e-02, -1.7306e-02,  7.5616e-03,\n",
      "         -8.4475e-02,  1.2621e-02,  9.2376e-02,  3.3410e-02,  5.6539e-02,\n",
      "          2.3897e-02, -1.2907e-02,  1.2019e-02,  1.1525e-01,  3.6434e-02,\n",
      "          1.7797e-04,  8.7522e-02,  1.6072e-02,  4.3492e-02, -4.3421e-02,\n",
      "         -2.2499e-04, -5.3580e-02, -2.7484e-02, -1.5940e-02,  4.3874e-02,\n",
      "         -4.6592e-02,  1.8726e-02,  4.3671e-03, -5.9352e-02, -2.4220e-02,\n",
      "          1.3807e-02, -4.1540e-02,  3.8926e-02,  9.1444e-02,  2.7211e-03,\n",
      "          6.4934e-02, -6.2555e-02, -5.3627e-02, -5.8046e-02, -2.0841e-02,\n",
      "          7.3592e-02,  5.9208e-02,  5.6832e-02, -7.9479e-02, -6.1730e-02,\n",
      "          7.2688e-03,  1.2688e-02,  6.6013e-02,  3.9855e-03,  4.4950e-03,\n",
      "          3.3733e-02,  4.4409e-02, -7.5419e-03, -6.9437e-02, -2.9886e-02,\n",
      "         -1.1938e-01, -1.2124e-02,  3.3952e-02,  8.6405e-02, -2.3087e-02,\n",
      "          6.4411e-02,  2.4891e-02, -4.8675e-02,  4.9256e-02,  3.0257e-02,\n",
      "          5.6126e-02, -1.1194e-03, -6.9972e-02,  3.9257e-02, -6.7335e-03,\n",
      "          7.5527e-02, -1.4873e-03, -2.6658e-02, -2.9059e-02, -5.5559e-02,\n",
      "          6.3677e-03, -6.3163e-02, -1.1852e-01, -4.9537e-02, -8.1666e-02,\n",
      "         -6.8841e-02, -6.6688e-02, -3.2569e-02,  4.4190e-02, -7.8997e-02,\n",
      "          1.2131e-02, -3.2941e-02,  7.2687e-02, -5.6978e-03, -7.3076e-02,\n",
      "         -1.4519e-04, -2.4226e-02,  2.5998e-02, -9.6840e-02,  3.6565e-02,\n",
      "          7.4108e-02, -3.8333e-02, -1.1482e-02,  3.3056e-02, -1.7439e-02,\n",
      "         -4.7849e-02,  3.8820e-03,  6.9266e-02,  5.0486e-02,  4.4679e-02,\n",
      "         -5.5232e-02, -1.4951e-02, -1.1610e-02, -3.0278e-03, -4.2579e-02,\n",
      "         -4.6914e-02, -2.3897e-03,  2.2151e-02, -9.3518e-02,  1.9729e-02,\n",
      "          6.8465e-02,  4.2101e-03,  2.3147e-02, -2.6177e-02, -5.3813e-02,\n",
      "         -5.6560e-04, -2.7088e-02,  6.9568e-02, -2.6084e-02, -3.4026e-02,\n",
      "          4.8696e-02, -2.8211e-02, -7.0117e-03, -2.7245e-02,  2.2563e-02,\n",
      "         -2.2459e-02, -3.0640e-02, -4.1486e-02, -2.6485e-02,  3.1090e-03,\n",
      "          6.9716e-02,  7.0826e-02, -4.2952e-02,  1.2239e-03, -4.0441e-02,\n",
      "          5.5233e-02,  8.0757e-03,  5.0300e-02,  2.6764e-02, -3.2989e-02,\n",
      "          7.9237e-02,  5.8645e-02,  1.0154e-01,  3.4954e-02,  8.1662e-03,\n",
      "          1.3593e-02, -7.8998e-02, -4.6426e-02, -1.8015e-02, -2.9066e-02,\n",
      "         -3.6370e-02,  4.0306e-02,  1.7079e-02,  9.3911e-02,  8.8070e-03,\n",
      "         -1.1840e-01,  2.2129e-02,  1.3037e-03, -6.0139e-02,  2.0020e-02,\n",
      "          1.6076e-02,  6.1511e-02,  2.4259e-03,  2.5339e-02, -3.7781e-02,\n",
      "         -3.1814e-02, -8.2972e-03,  7.5186e-03,  9.2962e-02, -3.6231e-02,\n",
      "         -4.0907e-02, -5.9201e-02,  4.6343e-02, -9.8854e-02,  5.9514e-03,\n",
      "          3.3740e-02, -4.4149e-02, -1.2026e-02,  5.7493e-02,  3.8476e-03,\n",
      "          1.3079e-02, -1.0449e-02,  4.2683e-03,  2.4374e-02, -4.5152e-02,\n",
      "         -1.9873e-02,  1.3350e-02,  7.1382e-02, -1.8681e-02,  1.9927e-02,\n",
      "         -3.4640e-03, -4.9116e-02, -9.0904e-02, -9.0065e-02,  3.0421e-02,\n",
      "         -5.9023e-02, -3.6600e-02, -8.7261e-03, -6.3028e-03,  1.7240e-02,\n",
      "          3.2915e-02, -1.1041e-02, -8.0923e-02,  2.2249e-02, -3.6304e-02,\n",
      "         -7.9205e-04, -1.4672e-02, -4.2622e-02, -1.7462e-02,  5.1110e-02,\n",
      "         -3.8082e-03,  4.4333e-02,  1.9734e-03,  6.0197e-02, -1.1411e-02,\n",
      "          1.8390e-02,  3.5534e-02, -6.4493e-03, -5.7961e-02, -2.7625e-02,\n",
      "         -5.5904e-02,  5.0711e-02,  9.1229e-03, -4.9901e-02, -6.8989e-02,\n",
      "         -4.1160e-02,  4.6939e-02,  1.4915e-02,  2.9150e-02,  5.8213e-02,\n",
      "         -9.1223e-04,  4.6086e-03,  4.8757e-02, -2.4870e-02,  5.5512e-03,\n",
      "         -9.5966e-03, -4.5517e-02,  3.6553e-02, -9.3806e-03,  1.6526e-03,\n",
      "         -2.9830e-02,  1.9758e-02,  3.7577e-03, -1.5326e-02, -1.2543e-02,\n",
      "          3.7661e-02, -7.5670e-02]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPConfig, AutoProcessor\n",
    "import torch\n",
    "\n",
    "def reduce_clip_layers(teacher_model: CLIPModel, num_text_layers: int, num_vision_layers: int) -> CLIPModel:\n",
    "    config = teacher_model.config.to_dict()\n",
    "    config['text_config']['num_hidden_layers'] = num_text_layers\n",
    "    config['vision_config']['num_hidden_layers'] = num_vision_layers\n",
    "    \n",
    "    student_config = CLIPConfig.from_dict(config)\n",
    "    \n",
    "    student_model = CLIPModel(student_config)\n",
    "    \n",
    "    teacher_text_layers = teacher_model.text_model.encoder.layers\n",
    "    student_text_layers = student_model.text_model.encoder.layers\n",
    "    \n",
    "    step_text = len(teacher_text_layers) // len(student_text_layers)\n",
    "    \n",
    "    for i, student_layer in enumerate(student_text_layers):\n",
    "        teacher_layer = teacher_text_layers[i * step_text]\n",
    "        student_layer.load_state_dict(teacher_layer.state_dict())\n",
    "    \n",
    "    teacher_vision_layers = teacher_model.vision_model.encoder.layers\n",
    "    student_vision_layers = student_model.vision_model.encoder.layers\n",
    "    \n",
    "    step_vision = len(teacher_vision_layers) // len(student_vision_layers)\n",
    "    \n",
    "    for i, student_layer in enumerate(student_vision_layers):\n",
    "        teacher_layer = teacher_vision_layers[i * step_vision]\n",
    "        student_layer.load_state_dict(teacher_layer.state_dict())\n",
    "    \n",
    "    return student_model\n",
    "\n",
    "teacher_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "num_text_layers = 2\n",
    "num_vision_layers = 2\n",
    "\n",
    "student_model = reduce_clip_layers(teacher_model, num_text_layers, num_vision_layers)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "text = [\"a photo of a dog\"]\n",
    "image = torch.rand(1, 3, 224, 224) \n",
    "\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = student_model(**inputs)\n",
    "\n",
    "text_features = outputs.text_embeds\n",
    "image_features = outputs.image_embeds\n",
    "\n",
    "print(\"Text Features:\", text_features)\n",
    "print(\"Image Features:\", image_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7122, 0.2878]])\n"
     ]
    }
   ],
   "source": [
    "# text=[\"a photo of a cat\", \"a photo of a dog\"]\n",
    "# # image = torch.rand(1, 3, 224, 224)\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"/home/ajeet/codework/datasets/video_incidents_ajeet/f044cf1d-8a5b-4703-a05c-3b57c5c14989_merged/0_34.jpg\")\n",
    "text = [\"a person\", \"a cell phone\"]\n",
    "\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = student_model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488.74753\n",
      "Model size in memory: 186.44 MB\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in student_model.parameters())\n",
    "param_size = 4\n",
    "\n",
    "print(total_params /100000)\n",
    "total_size = total_params * param_size / (1024 * 1024)\n",
    "print(f'Model size in memory: {total_size:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_quantize_clip(model: CLIPModel) -> CLIPModel:\n",
    "    model.text_model = torch.quantization.quantize_dynamic(\n",
    "        model.text_model,\n",
    "        {torch.nn.Linear},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    model.vision_model = torch.quantization.quantize_dynamic(\n",
    "        model.vision_model,\n",
    "        {torch.nn.Linear},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    return model\n",
    "\n",
    "quantized_model = dynamic_quantize_clip(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284.04481\n",
      "Model size in memory: 108.35 MB\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in quantized_model.parameters())\n",
    "param_size = 4\n",
    "\n",
    "print(total_params /100000)\n",
    "total_size = total_params * param_size / (1024 * 1024)\n",
    "print(f'Model size in memory: {total_size:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7193, 0.2807]])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"/home/ajeet/codework/datasets/video_incidents_ajeet/f044cf1d-8a5b-4703-a05c-3b57c5c14989_merged/0_34.jpg\")\n",
    "text = [\"a person\", \"a cell phone\"]\n",
    "\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = quantized_model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_visionwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
